{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readymade Data Module Assignment\n",
    "## NYC 311 Service Request Analysis\n",
    "\n",
    "**Student Name:** Nick Pisarczyk\n",
    "\n",
    "**U-M Unique Name:** npisar\n",
    "\n",
    "**Research Question:** State your chosen research question here\n",
    "\n",
    "---\n",
    "\n",
    "**BEFORE YOU START:**\n",
    "1. Read the assignment instructions on Canvas carefully.\n",
    "2. Make a copy of this notebook and work on your own copy.\n",
    "3. Understand the dataset before you start cleaning and analyzing. NYC Open Data has a nice portal and a data dictionary for exploring their datasets.\n",
    "4. NYC 311 is a **very large** dataset. When you are fetching data from the portal or API, we would recommend you to first think about your research questions and start with a small subset of the data and then increase the size of the data as you get more comfortable with the data. Generally, you do **not** need to use the entire dataset to answer your research question.\n",
    "5. This notebook serves as a template. You can add more cells or make adjustments as you see fit. But make sure to keep all the sections mentioned in the assignment instructions. Also, format your notebook properly for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statement\n",
    "\n",
    "Describe your data source here. Include:\n",
    "- Where you obtained the data (URL or API endpoint)\n",
    "- What subset you're analyzing (dates, geography, etc.)\n",
    "- Any filters or sampling you applied\n",
    "- File size/number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Details (Canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://umich.instructure.com/courses/825993/assignments/3016485\n",
    "\n",
    "**Overview**\n",
    "This assignment asks you to analyze NYC 311 service request data to understand patterns in urban neighborhoods. You will use Python to explore relationships between service requests and socioeconomic characteristics, practicing skills in data cleaning, exploratory analysis, visualization, and interpretation.\n",
    "\n",
    "311 systems allow residents to report non-emergency issues like noise complaints, street conditions, and building problems. As Wang et al. (2017) demonstrated, these service request patterns can reveal distinctive “signatures” of urban neighborhoods that correlate with demographic and economic characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives**<br>\n",
    "By completing this assignment, you will:<br>\n",
    "- Practice working with “readymade” administrative data from government sources\n",
    "- Develop skills in data cleaning and exploratory data analysis\n",
    "- Apply statistical methods to understand relationships in observational data\n",
    "- Create effective visualizations to communicate data patterns\n",
    "- Consider methodological limitations and ethical implications of using administrative data for research\n",
    "- Data\n",
    "\n",
    "---\n",
    "\n",
    "**NYC Open Data - 311 Service Requests**\n",
    "\n",
    "You will use NYC’s publicly available 311 service request dataLinks to an external site..\n",
    "\n",
    "Recommended approach: Explore the dataset using the NYC Open Data online portal and the data dictionary. The full dataset is very large (40+ million records), so filtering is essential. For instance, one year of data was around 2.6 GB. \n",
    "\n",
    "Based on your research question, identify an appropriate subset of the data (e.g., one year of data for specific boroughs or zip codes), and use the Socrata API or directly download it.You can find more information about the API and the documentation on NYC Open Data website.\n",
    "\n",
    "---\n",
    "\n",
    "**Research Questions**\n",
    "Choose ONE of the following research questions to investigate:\n",
    "\n",
    "- Option 1: Neighborhood Service Request Signatures\n",
    "<br>Do different neighborhoods have distinctive patterns in their 311 service requests? Analyze the relative frequency of complaint types across neighborhoods. What patterns do you notice?\n",
    "\n",
    "- Option 2: Temporal Patterns in Service Requests\n",
    "<br>How do 311 service requests vary by time of day, day of week, or season? Are there particular complaint types that show strong temporal patterns? What might explain these patterns?\n",
    "\n",
    "- Option 3: Response Time Disparities\n",
    "<br>Do response times for 311 requests differ across neighborhoods or complaint types? Calculate the time between request creation and closure, and examine whether there are systematic differences by location or complaint category.\n",
    "\n",
    "- Option 4: Complaint Type Evolution\n",
    "<br>How have the types and volumes of 311 requests changed over time (comparing multiple years)? What might explain these patterns?\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment Requirements**<br>\n",
    "Your submission should include two components. Both should be included in a single Jupyter Notebook.\n",
    "<br>\n",
    "1. Data Analysis<br>\n",
    "Your notebook should include:\n",
    "\n",
    "- Data loading and initial exploration: Load the data, explain how you retrieve the data, why you choose the specific subset, examine its structure, and identify relevant variables\n",
    "- Data cleaning: Handle missing values, filter to relevant records, create derived variables as needed\n",
    "- Exploratory analysis: Use descriptive statistics and visualizations to understand patterns\n",
    "- Focused analysis: Apply appropriate statistical methods to address your research question\n",
    "- Visualizations: Create at least 3 meaningful visualizations (e.g., time series, bar charts, heatmaps, scatter plots)\n",
    "- Documentation: Include markdown cells explaining your approach, interpreting results, and noting limitations\n",
    "- Technical expectations: Use numpy/pandas for data manipulation, matplotlib/seaborn for visualization, and appropriate statistical libraries (scipy, statsmodels, etc.) as needed.\n",
    "\n",
    "2. Written Summary<br>\n",
    "Briefly report on the following. Your notebook should have a section for each bullet point.\n",
    "\n",
    "- Research question and motivation: What is your research question? Why is this question interesting? What might we learn?\n",
    "- Methods: Describe your data source, cleaning steps, and analytical approach\n",
    "- Findings: Summarize key patterns and statistical results (include 2-3 key visualizations)\n",
    "- Limitations: What are the methodological limitations? What can and cannot be concluded from this analysis?\n",
    "- Ethical considerations: Reflect on the use of 311 data for research. Who is represented? Who might be excluded? What are potential concerns about using this data?\n",
    " \n",
    "---\n",
    "\n",
    "**Assignment Submission**\n",
    "\n",
    "Please upload your Jupyter Notebook file to Canvas by the deadline: A single .ipynb file that includes all your work. \n",
    "\n",
    "Before submitting the file, ensure that your notebook is properly running on your machine. Ensure your notebook contains all your responses, plots, all programming parts of this assignment and is properly formatted with headings, explanations and code comments.\n",
    "\n",
    "If you use Generative AI tools in any part of your assignment, you need to follow the AI policy in our syllabus and document your AI use at the end of your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# You can import any libraries you may need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv helper function\n",
    "def df_to_csv(d):\n",
    "    for borough, df in d.items():\n",
    "        df.to_csv(f'data/readymade_module_data/{borough}_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Load from downloaded CSV file\n",
    "\n",
    "**Note**: If you are running the notebook using `colab` kernel, you **cannot** directly import the data from your own laptop. Please see the class repo README files for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your downloaded data\n",
    "# df = pd.read_csv('311_Service_Requests.csv', \n",
    "#                  parse_dates='Created Date', 'Closed Date',\n",
    "#                  low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load from Socrata API (recommended for smaller datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data for borough: BRONX...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request.\n\tQuery coordinator error: query.soql.no-such-column; No such column: BRONX; position: Map(row -> 1, column -> 167, line -> \"SELECT `unique_key`, `created_date`, `closed_date`, `complaint_type`, `descriptor`, `descriptor_2`, `borough` WHERE (`created_date` >= \\\"2026-01-01\\\") AND (`borough` = `BRONX`) LIMIT 100000\\n                                                                                                                                                                      ^\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m borough \u001b[38;5;129;01min\u001b[39;00m boroughs:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGathering data for borough: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mborough\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     results = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merm2-nwe9\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mselect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselect_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreated_date >= \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2026-01-01\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AND borough = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mborough\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     dfs[borough.lower()] = pd.DataFrame.from_records(results)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting time columns...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\School\\WN 2026\\SI313-homework\\.venv\\Lib\\site-packages\\sodapy\\socrata.py:412\u001b[39m, in \u001b[36mSocrata.get\u001b[39m\u001b[34m(self, dataset_identifier, content_type, **kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m params.update(kwargs)\n\u001b[32m    410\u001b[39m params = utils.clear_empty_values(params)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\School\\WN 2026\\SI313-homework\\.venv\\Lib\\site-packages\\sodapy\\socrata.py:555\u001b[39m, in \u001b[36mSocrata._perform_request\u001b[39m\u001b[34m(self, request_type, resource, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;66;03m# handle errors\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m200\u001b[39m, \u001b[32m202\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m     \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# when responses have no content body (ie. delete, set_permission),\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# simply return the whole response\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\School\\WN 2026\\SI313-homework\\.venv\\Lib\\site-packages\\sodapy\\utils.py:30\u001b[39m, in \u001b[36mraise_for_status\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m more_info \u001b[38;5;129;01mand\u001b[39;00m more_info.lower() != response.reason.lower():\n\u001b[32m     29\u001b[39m     http_error_msg += \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(more_info)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m requests.exceptions.HTTPError(http_error_msg, response=response)\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request.\n\tQuery coordinator error: query.soql.no-such-column; No such column: BRONX; position: Map(row -> 1, column -> 167, line -> \"SELECT `unique_key`, `created_date`, `closed_date`, `complaint_type`, `descriptor`, `descriptor_2`, `borough` WHERE (`created_date` >= \\\"2026-01-01\\\") AND (`borough` = `BRONX`) LIMIT 100000\\n                                                                                                                                                                      ^\")"
     ]
    }
   ],
   "source": [
    "# Example: Get 100,000 records from 2023 for Brooklyn\n",
    "client = Socrata(\"data.cityofnewyork.us\", None)\n",
    "\n",
    "# specify columns to reduce runtimes and get only pertinent data\n",
    "columns = [\n",
    "    \"unique_key\",\n",
    "    \"created_date\",\n",
    "    \"closed_date\",\n",
    "    \"complaint_type\",\n",
    "    \"descriptor\",\n",
    "    \"descriptor_2\",\n",
    "    \"borough\"\n",
    "]\n",
    "select_str = ', '.join(columns)\n",
    "\n",
    "# define the 5 NYC boroughs (neighborhoods)\n",
    "boroughs = [\n",
    "    \"BRONX\",\n",
    "    \"BROOKLYN\",\n",
    "    \"MANHATTAN\",\n",
    "    \"QUEENS\",\n",
    "    \"STATEN ISLAND\"\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "# find data for each borough and create 5 separate dfs\n",
    "for borough in boroughs:\n",
    "    print(f\"Gathering data for borough: {borough}...\")\n",
    "    results = client.get(\"erm2-nwe9\", \n",
    "                        select=select_str,\n",
    "                        where=f\"created_date >= '2026-01-01' AND borough = {borough}\",\n",
    "                        limit=100000)\n",
    "    dfs[borough.lower()] = pd.DataFrame.from_records(results)\n",
    "\n",
    "    print(f\"Converting time columns...\")\n",
    "    # Convert date columns\n",
    "    dfs[borough.lower()]['created_date'] = pd.to_datetime(dfs[borough.lower()]['created_date'])\n",
    "    dfs[borough.lower()]['closed_date'] = pd.to_datetime(dfs[borough.lower()]['closed_date'])\n",
    "\n",
    "    if not borough == \"STATEN ISLAND\":\n",
    "        print(f\"Complete! Next bourough...\\n\\n\")\n",
    "    else:\n",
    "        print(f\"Complete for all boroughs!\")\n",
    "    \n",
    "print(f\"dfs dict is:\\n{dfs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to CSV for easier access\n",
    "df_to_csv(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Description\n",
    "\n",
    "You can describe the data in many ways. Here are some baseline requirements:\n",
    "- Display basic information about the dataset (what are the relevant variables? What are their types? How many observations are there?)\n",
    "- Conduct summary statistics of the relevant variables\n",
    "- Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   unique_key      100000 non-null  str           \n",
      " 1   created_date    100000 non-null  datetime64[us]\n",
      " 2   closed_date     91957 non-null   datetime64[us]\n",
      " 3   complaint_type  100000 non-null  str           \n",
      " 4   descriptor      100000 non-null  str           \n",
      " 5   city            96605 non-null   str           \n",
      " 6   borough         100000 non-null  str           \n",
      " 7   descriptor_2    52233 non-null   str           \n",
      "dtypes: datetime64[us](2), str(6)\n",
      "memory usage: 6.1 MB\n"
     ]
    }
   ],
   "source": [
    "# You can have as many cells as you want\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "Document your cleaning decisions and rationale here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example cleaning steps (customize based on your needs)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Remove rows with missing essential data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# df_clean'day_of_week' = df_clean'created_date'.dt.dayofweek\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# df_clean'month' = df_clean'created_date'.dt.month\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Example cleaning steps (customize based on your needs)\n",
    "\n",
    "# 1. Remove rows with missing essential data\n",
    "# df_clean = df.dropna(subset='created_date', 'complaint_type')\n",
    "\n",
    "# 2. Filter to specific time period if needed\n",
    "# df_clean = df_clean(df_clean'created_date' >= '2023-01-01') & \n",
    "#                     (df_clean'created_date' < '2024-01-01')\n",
    "\n",
    "# 3. Create derived variables\n",
    "# Example: Calculate response time\n",
    "# df_clean'response_time_hours' = (\n",
    "#     (df_clean'closed_date' - df_clean'created_date').dt.total_seconds() / 3600\n",
    "# )\n",
    "\n",
    "# Example: Extract temporal features\n",
    "# df_clean'hour' = df_clean'created_date'.dt.hour\n",
    "# df_clean'day_of_week' = df_clean'created_date'.dt.dayofweek\n",
    "# df_clean'month' = df_clean'created_date'.dt.month\n",
    "\n",
    "print(f\"Original dataset: {len(df)} records\")\n",
    "# print(f\"Cleaned dataset: {len(df_clean)} records\")\n",
    "# print(f\"Removed: {len(df) - len(df_clean)} records ({((len(df) - len(df_clean))/len(df)*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Exploratory Data Analysis\n",
    "\n",
    "Add narrative about what you're exploring, why and what you've found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Most common complaint types\n",
    "# complaint_counts = df_clean'complaint_type'.value_counts().head(15)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# complaint_counts.plot(kind='barh')\n",
    "# plt.xlabel('Number of Requests')\n",
    "# plt.ylabel('Complaint Type')\n",
    "# plt.title('Top 15 Most Common 311 Complaint Types')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Temporal patterns\n",
    "# requests_by_month = df_clean.groupby('month').size()\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# requests_by_month.plot(kind='bar')\n",
    "# plt.xlabel('Month')\n",
    "# plt.ylabel('Number of Requests')\n",
    "# plt.title('311 Requests by Month')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Research Question Analysis\n",
    "\n",
    "This is the core of your assignment. Document your analytical approach here. You can add any cells if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your focused analysis goes here\n",
    "# This will vary significantly based on your research question. You can organize your analysis as you like. You can have as many cells as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Statistical tests\n",
    "# from scipy import stats\n",
    "\n",
    "# # t-test example\n",
    "# group1 = df_cleandf_clean'borough' == 'MANHATTAN''response_time_hours'.dropna()\n",
    "# group2 = df_cleandf_clean'borough' == 'BRONX''response_time_hours'.dropna()\n",
    "# t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "# print(f\"t-statistic: {t_stat:.3f}\")\n",
    "# print(f\"p-value: {p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Visualizations\n",
    "\n",
    "Create at least 3 polished visualizations that answer your research question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Visualization 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Summary\n",
    "\n",
    "Summarize your key findings here. What patterns did you discover? What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question and Motivation\n",
    "\n",
    "- Why is this question interesting? \n",
    "- What might we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "Describe your data source, cleaning steps, and analytical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "Summarize key patterns and statistical results (refer to your key visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "Discuss methodological limitations. What are the potential biases in 311 data? What alternative explanations exist for your findings? What can and cannot be concluded from this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Considerations\n",
    "\n",
    "Reflect on the ethical implications of using 311 data:\n",
    "- Who is represented in this data? Who might be underrepresented?\n",
    "- What are potential privacy concerns?\n",
    "- How might this analysis be used or misused?\n",
    "- What are the implications for equity and justice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Appendix (if applicable)\n",
    "\n",
    "If you used AI during this assignnment, explain\n",
    "1. what part of the work it was used for; \n",
    "2. what AI tools you used; \n",
    "3. the prompts you used; \n",
    "4. how you analyzed the AI work for accuracy; and, \n",
    "5. steps you took to rework and revise your final documents so that they were both factually accurate and reflected your own voice and style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit your assignment as .ipynb file. Make sure to double check with the assignment instructions on Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
